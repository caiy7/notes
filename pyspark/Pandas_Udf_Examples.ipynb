{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF\n",
    "This notebook demostrates how to use pandas_udf function to work with pyspark DataFrame with a flavor of pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/ying/spark-2.3.2-bin-hadoop2.7/')  #provide spark path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use `pandas_udf` and PandasUDFType from `pyspark.sql.functions`. As pandas_udf was built on top of Apache Arrow, make sure pyarrow is installed. To install pyarrow  \n",
    "**Conda**:\n",
    "```bash\n",
    "conda install -c conda-forge pyarrow\n",
    "```\n",
    "Or **Pip**:\n",
    "```bash\n",
    "pip install pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not show warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SparkSession for DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data. Here I am using a subset of movie rating downloaded from MovieLens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('rating_10000.csv', sep=',', inferSchema=True,\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove timestamp column since I am not going to work with it.\n",
    "df = df.drop('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make a copy of pandas dataframe from spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Demeaning rating using Scalar Pandas UDFs\n",
    "Group the dataframe by `userId` and subtract each rating by the mean rating within each user group. As we will only use `rating` to calculate the `demeaned_rating`, we will use the Scalar Pandas UDF, which takes a column and return a column. \n",
    "The function is exactly the same function we will use with a pandas dataframe. We just add a pandas_udf decorator with specified `returnType` and `functionType`.\n",
    "\n",
    "First, let see how we do it with pandas dataframe - define a demean function and use it with `withColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean(rating):\n",
    "    return rating - rating.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['demean'] = pdf['rating'].transform(demean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>demean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9761</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.49275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9761</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.49275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9761</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.49275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9761</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.49275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9761</td>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.49275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9761</td>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.50725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9761</td>\n",
       "      <td>32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.50725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   demean\n",
       "0    9761       11     3.0 -0.49275\n",
       "1    9761       14     2.0 -1.49275\n",
       "2    9761       17     3.0 -0.49275\n",
       "3    9761       19     1.0 -2.49275\n",
       "4    9761       21     3.0 -0.49275\n",
       "5    9761       31     4.0  0.50725\n",
       "6    9761       32     4.0  0.50725"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use pandas_udf to work with spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def demean(rating):\n",
    "    return rating - rating.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demean = df.withColumn('demeaned_rating', demean('rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------------+\n",
      "|userId|movieId|rating|demeaned_rating|\n",
      "+------+-------+------+---------------+\n",
      "|  9761|     11|   3.0|       -0.49275|\n",
      "|  9761|     14|   2.0|       -1.49275|\n",
      "|  9761|     17|   3.0|       -0.49275|\n",
      "|  9761|     19|   1.0|       -2.49275|\n",
      "|  9761|     21|   3.0|       -0.49275|\n",
      "|  9761|     31|   4.0|        0.50725|\n",
      "|  9761|     32|   4.0|        0.50725|\n",
      "+------+-------+------+---------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demean.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually get the demeaned rating by passing the expression directly to `withColumn` (as shown below). Just wanted to show how Scalar Pandas UDF works. We will also see how the Grouped Map Pandas UDFs works with `groupBy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------+\n",
      "|userId|movieId|rating|demeaned|\n",
      "+------+-------+------+--------+\n",
      "|  9761|     11|   3.0|-0.49275|\n",
      "|  9761|     14|   2.0|-1.49275|\n",
      "|  9761|     17|   3.0|-0.49275|\n",
      "|  9761|     19|   1.0|-2.49275|\n",
      "|  9761|     21|   3.0|-0.49275|\n",
      "|  9761|     31|   4.0| 0.50725|\n",
      "|  9761|     32|   4.0| 0.50725|\n",
      "+------+-------+------+--------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('demeaned', df['rating']-df.select(avg('rating')).collect()[0][0]).show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Demeaning rating within each user group using Group Map Pandas UDF\n",
    "When define the function, we pass a pandas dataframe as argument and return a pandas dataframe. We also need to pass the `returnType` and `functionType`, which is `PandasUDFType.GROUPED_MAP` in this case, to the function decorator. (In [spark 2.4.0](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#grouped-aggregate), `PandasUDFType.GROUPED_AGG` is also available for aggregation operations.) Then we can use this funnction with `groupBy` and `apply`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "demean_schema = 'userId int, movieId int, rating double, user_demeaned_rating double'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(demean_schema, PandasUDFType.GROUPED_MAP)\n",
    "def demean_user(df):  #df is pandas dataframe\n",
    "    df['user_demeaned_rating'] = df['rating'] - df['rating'].mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_demean = df.groupBy('userId').apply(demean_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+\n",
      "|userId|movieId|rating|user_demeaned_rating|\n",
      "+------+-------+------+--------------------+\n",
      "| 14832|      1|   3.0| -0.3076923076923075|\n",
      "| 14832|      2|   4.0|  0.6923076923076925|\n",
      "| 14832|      5|   3.0| -0.3076923076923075|\n",
      "| 14832|     34|   3.0| -0.3076923076923075|\n",
      "| 14832|     48|   4.0|  0.6923076923076925|\n",
      "| 14832|     60|   3.0| -0.3076923076923075|\n",
      "| 14832|    107|   3.0| -0.3076923076923075|\n",
      "+------+-------+------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_demean.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Perform train test split within each user group using Group Map Pandas UDF\n",
    "In this task, I will pick about half of the users and take 10 movie ratings from each if the user has more than 25 ratings. I will make a new column `istest` to indicate if the rating is picked (1 otherwise 0) and then split the dataset into traing and test sets. Group Map Pandas UDF is really handy in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "istest_schema = 'userId int, movieId int, rating double, istest int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MOVIES=10\n",
    "#To define pandas_udf to split the data within user\n",
    "@pandas_udf(istest_schema, PandasUDFType.GROUPED_MAP)\n",
    "def istest(df):\n",
    "    '''\n",
    "    Select around 40% of users in the dataset as test users.\n",
    "    take n movies from test user that has more than 25 ratings as test dataset.\n",
    "    return train, test dataset'''   \n",
    "    is_test_user = np.random.random()  #assign a random number and only pick ratings if it is > 0.5\n",
    "    new_test = np.zeros(len(df))\n",
    "    if is_test_user > 0.5 and len(df)> 25:      \n",
    "        test_ind = np.random.choice(np.arange(len(df)), TEST_MOVIES, replace=False)\n",
    "        new_test[test_ind]=1\n",
    "    df['istest'] = new_test\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ind = df.groupBy('userId').apply(istest).cache()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the lazy evaluation property of spark, the split result will change everytime we call action operation like `show()` or `count()`. Use `cache()` to storage it in memory for the sake of consistency in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|istest|count|\n",
      "+------+-----+\n",
      "|     1|  270|\n",
      "|     0| 9730|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_ind.groupBy('istest').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset according to the 'istest' column\n",
    "train = df_with_ind.filter('istest=0')\n",
    "test = df_with_ind.filter('istest=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9730, 270)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
